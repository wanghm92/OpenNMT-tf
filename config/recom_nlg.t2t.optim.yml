# Transformer configuration for single GPU training.

model_dir: ../onmt-tf-output/recom_nlg_clothes_t2t/layer2-unit256-head8-ffn1024_filter/

params:
  optimizer: LazyAdamOptimizer
  learning_rate: 2.0  # The scale constant.
  decay_type: noam_decay
  decay_rate: 512  # Model dimension.
  decay_steps: 4000  # Warmup steps.

  # Divide this value by the total number of GPUs used.
  decay_step_duration: 8  # 1 decay step is 8 training steps.

  average_loss_in_time: true
  label_smoothing: 0.1

  length_penalty: 0.6

train:
  batch_size: 1536
  batch_type: tokens
  bucket_width: 1

  train_steps: 1000000

  # Consider setting this to -1 to match the number of training examples.
  sample_buffer_size: -1

eval:
  batch_size: 64
  eval_delay: 1800  # Every 5 hours.

infer:
  batch_size: 32
  n_best: 4